{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P-Twitter_API.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTCdQoSuQJUF"
      },
      "source": [
        "## **PRODUÇÃO - A (LOAD)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BYw7mGCQZHU"
      },
      "source": [
        "# pandas para manipulação de dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
        "\n",
        "# pacotes de visualização de dados\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
        "\n",
        "# pacotes para estatísticas e criação de modelos\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# CLUSTER\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# PCA \n",
        "from sklearn.decomposition import PCA\n",
        "# Import the KElbowVisualizer method \n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
        "\n",
        "# upload de arquivos \n",
        "from google.colab import files\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
        "\n",
        "!pip install  -q pytz\n",
        "import pytz\n",
        "from datetime import datetime, timezone\n",
        "from pytz import all_timezones\n",
        "\n",
        "'''\n",
        "for tz in pytz.all_timezones:\n",
        "    print (tz)\n",
        "'''\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
        "# Configurando o número máximo de linhas a se mostrar\n",
        "pd.set_option('display.max_row', 5000)\n",
        "\n",
        "# Configurando o número máximo de colunas a se mostrar\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "# Aumentando o número de caracteres a serem exibidos numa coluna de texto\n",
        "pd.options.display.max_colwidth = 500\n",
        "\n",
        "# Desligando a notificação setcopywarning - Já contolamos a situação\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "import datetime as DT\n",
        "import io\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import googleapiclient.discovery\n",
        "import numpy as np \n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "# Set ipython's max row display\n",
        "pd.set_option('display.max_row', 10000)\n",
        "\n",
        "# Set iPython's max column width to 50\n",
        "pd.set_option('display.max_columns', 150)\n",
        "pd.options.display.max_colwidth = 500\n",
        "\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "!pip install -U -q tweepy\n",
        "!pip install -U -q textblob\n",
        "!pip install -U -q TwitterAPI\n",
        "from TwitterAPI import TwitterAPI, TwitterOAuth, TwitterRequestError, TwitterConnectionError, TwitterPager\n",
        "\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "\n",
        "!pip install  -q nltk\n",
        "!pip install  -q unidecode\n",
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('portuguese'))\n",
        "print(stopwords)\n",
        "!pip install  -q emojis\n",
        "import emojis\n",
        "\n",
        "'''\n",
        "consumer_key = \"XXXXXXXXXXXXXXXXXXXXX\" \n",
        "consumer_secret = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        "access_key = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        "access_secret = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        "'''\n",
        "\n",
        "# Twitter authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)   \n",
        "auth.set_access_token(access_key, access_secret) \n",
        "  \n",
        "# Creating an API object \n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "api\n",
        "\n",
        "api_v2 = TwitterAPI(consumer_key, consumer_secret, access_key, access_secret, api_version='2')\n",
        "\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDxCRT0QXrT"
      },
      "source": [
        "## **PRODUÇÃO - B (RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lekJ-sP4QYB6"
      },
      "source": [
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==================================== TWEETS DOS TIMES ATRAVÉS DO TWEEPY\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "df_schedule0 = pd.read_csv('/content/drive/My Drive/futebol_emocoes/times_do_brasileirao.csv', error_bad_lines=False, sep=',')\n",
        "df_schedule0\n",
        "df_schedule0.head(20)\n",
        "\n",
        "df_schedule1 = df_schedule0[:2]\n",
        "df_schedule1\n",
        "\n",
        "tweet_list = []\n",
        "\n",
        "for games in df_schedule0.itertuples():\n",
        "\n",
        "  nome            = getattr(games, 'nome')\n",
        "  perfil_twitter  = getattr(games, 'perfil_twitter') \n",
        "  dt              = getattr(games, 'dt')\n",
        "  hr_ini          = getattr(games, 'hr_ini')\n",
        "  hr_fim          = getattr(games, 'hr_fim')\n",
        "  \n",
        "  print(nome)\n",
        "  #new_tweets = tweepy.Cursor(api.user_timeline, screen_name=\"Flamengo\", tweet_mode='extended', since=1399102279203102721).items(100)\n",
        "  new_tweets = tweepy.Cursor(api.user_timeline, screen_name=perfil_twitter, tweet_mode='extended', until=dt).items(100)\n",
        "\n",
        "\n",
        "  for tweet_obj in new_tweets:\n",
        "      #print(tweet_obj)\n",
        "\n",
        "      #print(tweet_obj.user.screen_name)\n",
        "      user_screen_name = tweet_obj.user.screen_name\n",
        "      id = tweet_obj.id\n",
        "      created_at = tweet_obj.created_at\n",
        "      full_text = tweet_obj.full_text\n",
        "      followers_count = tweet_obj.user.followers_count\n",
        "      friends_count = tweet_obj.user.friends_count\n",
        "      listed_count = tweet_obj.user.listed_count\n",
        "      favorite_count = tweet_obj.favorite_count\n",
        "      retweet_count = tweet_obj.retweet_count\n",
        "      \n",
        "      #HASHTAGS\n",
        "      try:\n",
        "          hashtags_list = tweet_obj['entities']['hashtags']\n",
        "          hashtags = [hashtag['text'] for hashtag in hashtags_list]\n",
        "          if len(hashtags_list)>1:\n",
        "              temp['hashtags'] = \",\".join(hashtags)\n",
        "          else:\n",
        "              hashtags = hashtags_list[0]['text']\n",
        "      except:\n",
        "              hashtags = ''\n",
        "\n",
        "      #LOCALIZAÇÃO\n",
        "      try:\n",
        "          location = tweet_obj['user']['location']\n",
        "      except:\n",
        "          location = None\n",
        "      \n",
        "      #URL\n",
        "      try:\n",
        "          url = tweet_obj['entities']['urls'][0]['url']\n",
        "      except:\n",
        "          url = ''\n",
        "\n",
        "      d = {'nome_time': nome, 'screen_name': perfil_twitter, 'dt_jogo': dt, 'hr_ini': hr_ini, 'hr_fim': hr_fim,\n",
        "          'user_screen_name': user_screen_name, 'id': id, 'created_at': created_at, 'full_text': full_text, 'followers_count': followers_count, \n",
        "          'friends_count': friends_count, 'listed_count': listed_count, 'hashtags': hashtags, 'location': location, 'url': url, \n",
        "          'favorite_count': favorite_count, 'retweet_count': retweet_count }         \n",
        "      #print(d)\n",
        "      ooo = pd.DataFrame(d, index=[0])\n",
        "\n",
        "      ooo['hora_brasil'] = pd.DatetimeIndex(pd.to_datetime(ooo['created_at'],unit='ms')).tz_localize('UTC').tz_convert('Brazil/East')\n",
        "      ooo['dt'] = ooo['hora_brasil'].dt.date\n",
        "\n",
        "      ooo = (ooo.set_index('hora_brasil').between_time(hr_ini, hr_fim).reset_index().reindex(columns=ooo.columns))\n",
        "\n",
        "      url = \"/content/drive/My Drive/futebol_emocoes/\"\n",
        "      ext   = '.csv'\n",
        "      hifen = \"_\"\n",
        "      final_url = f'{url}{nome}{hifen}{dt}{ext}'\n",
        "      ooo.to_csv(final_url)\n",
        "\n",
        "      tweet_list.append(ooo) \n",
        "\n",
        "\n",
        "df_game_plays = pd.concat(tweet_list)\n",
        "df_game_plays.head(2000)\n",
        "df_game_plays.info()\n",
        "dia_de_jogo = df_game_plays\n",
        "dia_de_jogo\n",
        "\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==================================== EXTRAÇÃO DOS CONVERSATION ID´S - TWITTER API V2\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "conv_ids = []\n",
        " \n",
        "for row in dia_de_jogo.itertuples():\n",
        "        id          = getattr(row, 'id')\n",
        "        nome_time   = getattr(row, 'nome_time')\n",
        "\n",
        "        TWEET_ID = id\n",
        "        TWEET_FIELDS = 'conversation_id'\n",
        "        \n",
        "        try:\n",
        "            r = api_v2.request(f'tweets/:{TWEET_ID}', {'tweet.fields': TWEET_FIELDS})\n",
        "\n",
        "            for item in r:\n",
        "\n",
        "                  conversation_id   = item['conversation_id']\n",
        "                  original_tweet_id = item['id']\n",
        "                  #print(item)\n",
        "                  #print(item['conversation_id'])\n",
        "                  #print(item['id'])\n",
        "\n",
        "                  d = {'nome_time': nome_time, 'original_tweet_id': original_tweet_id, 'conversation_id': conversation_id}         \n",
        "                    \n",
        "                  ooo = pd.DataFrame(d, index=[0])  \n",
        "                  conv_ids.append(ooo)\n",
        "\n",
        "        except TwitterRequestError as e:\n",
        "            print(e.status_code)\n",
        "            for msg in iter(e):\n",
        "                print(msg)\n",
        "\n",
        "        except TwitterConnectionError as e:\n",
        "            print(e)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "df_conversation_ids = pd.concat(conv_ids)\n",
        "df_conversation_ids\n",
        "df_conversation_ids['conversation_id_num'] = df_conversation_ids['conversation_id'].astype(int)\n",
        "df_conversation_ids\n",
        "\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==================================== EXTRAÇÃO DAS RESPOSTAS - TWITTER API V2\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "\n",
        "replies = []\n",
        "user=[]\n",
        "for row in df_conversation_ids.itertuples():\n",
        "   conversation_id_num = getattr(row, 'conversation_id_num')\n",
        "   original_tweet_id   = getattr(row, 'original_tweet_id')\n",
        "   nome_time           = getattr(row, 'nome_time')   \n",
        "   #print(conversation_id_num)\n",
        "\n",
        "   headers = {'Authorization': \"Bearer \" + str('YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY') }\n",
        "   url = \"https://api.twitter.com/2/tweets/search/recent?query=\"\n",
        "   conversation = \"conversation_id:\"\n",
        "   tweet_fieds = \"&tweet.fields=id,text,source,created_at,author_id\"\n",
        "   user = \"&expansions=author_id,geo.place_id&user.fields=id,name,username,created_at,description,public_metrics\"\n",
        "   place = \"&place.fields=country,full_name,geo,name,place_type\"    \n",
        "   max_results=\";max_results=100\"\n",
        "   #final_url = f'{url}{conversation}{conversation_id_num}{tweet_fieds}{place}{max_results}'\n",
        "   final_url = f'{url}{conversation}{conversation_id_num}{tweet_fieds}{user}{place}{max_results}'\n",
        "   #print(final_url)\n",
        "   try:\n",
        "        response = requests.get( final_url, headers=headers)\n",
        "        response_data = (response.json())\n",
        "        \n",
        "        #print(response_data)\n",
        "        for n in range(len(response_data['data'])):\n",
        "            #print(n)\n",
        "            tweet                = response_data['data'][n]['text']\n",
        "            original_tweet_id    = original_tweet_id\n",
        "            conversation_id      = conversation_id_num\n",
        "            created_at           = response_data['data'][n]['created_at']                        \n",
        "            reply_user_id        = response_data['data'][n]['author_id']\n",
        "            reply_source         = response_data['data'][n]['source']\n",
        "          \n",
        "            if \"geo\" in response_data['data'][n]:\n",
        "                      geo_id            = response_data['data'][n]['geo']['place_id']\n",
        "                      #print(response_data['data'][n]['geo']['place_id'])\n",
        "            else: \n",
        "                      geo_id            = np.nan  \n",
        "          \n",
        "            d = {'nome_time': nome_time, 'original_tweet_id': original_tweet_id, 'conversation_id': conversation_id, 'tweet_text': tweet, \n",
        "                'created_at': created_at, 'reply_user_id': reply_user_id, 'geo_id': geo_id, 'reply_source': reply_source}     \n",
        "            \n",
        "            #print(nome_time,conversation_id) \n",
        "            ooo = pd.DataFrame(d, index=[0])   \n",
        "            replies.append(ooo) \n",
        "        time.sleep(2)\n",
        "   except:\n",
        "        pass\n",
        "df_replies0 = pd.concat(replies)\n",
        "df_replies0\n",
        "\n",
        "df_replies0.to_csv('/content/drive/My Drive/futebol_emocoes/rodada_1.csv')\n",
        "\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==================================== PRÉ TRATAMENTO DOS TWEETS \n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "# cópia de segurança\n",
        "df_replies0_bkp = df_replies0\n",
        "# df_replies0 = df_replies0_bkp\n",
        "\n",
        "#remove números e caracteres especiais \n",
        "df_replies0['tweet_text'] = df_replies0['tweet_text'].apply(lambda x: re.sub('[0-9]!\"#$%&\\(\\)*\\+,-\\./:;<=>?@\\\\\\^_`{\\|}~', ' ', x))\n",
        "df_replies0\n",
        "\n",
        "# CONVERTE TODAS AS LETRAS PARA MINÚSCULO\n",
        "\n",
        "df_replies0['tweet_text'] = df_replies0['tweet_text'].apply(lambda x: x.lower())\n",
        "\n",
        "# EXTRAINDO HASHTAGS\n",
        "\n",
        "df_replies0['hashtag'] = df_replies0['tweet_text'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
        "\n",
        "# EXTRAINDO MENÇÕES\n",
        "\n",
        "df_replies0['mentions'] = df_replies0['tweet_text'].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
        "\n",
        "# EXTRAINDO EMOJIS\n",
        "\n",
        "df_replies0['emojis']  = df_replies0['tweet_text'].apply(lambda x: emojis.get(x))\n",
        "\n",
        "#\n",
        "# REMOVING LINKS\n",
        "#\n",
        "\n",
        "df_replies0['tweet_semlinks'] = df_replies0['tweet_text'].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", x))\n",
        "df_replies0.head(2000)\n",
        "\n",
        "#\n",
        "# REMOVING MENTIONS\n",
        "#\n",
        "\n",
        "df_replies0['tweet_semmencoes'] = df_replies0['tweet_semlinks'].apply(lambda x: re.sub('@[\\w]+',\"\", x))\n",
        "df_replies0\n",
        "\n",
        "#\n",
        "# REMOVING EMOJIS\n",
        "#\n",
        "\n",
        "df_replies0['tweet_sememojis'] = df_replies0['tweet_semmencoes'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
        "df_replies0\n",
        "\n",
        "stopwords = [\"a\",\"agora\",\"ainda\",\"alguém\",\"algum\",\"alguma\",\"algumas\",\"alguns\",\"ampla\",\"amplas\",\"amplo\",\"amplos\",\"ante\",\"antes\",\"ao\",\"aos\",\"após\",\"aquela\",\"aquelas\",\"aquele\",\"aqueles\",\n",
        "\"aquilo\",\"as\",\"até\",\"através\",\"cada\",\"coisa\",\"coisas\",\"com\",\"como\",\"contra\",\"contudo\",\"da\",\"daquele\",\"daqueles\",\"das\",\"de\",\"dela\",\"delas\",\"dele\",\"deles\",\"depois\",\"dessa\",\"dessas\",\n",
        "\"desse\",\"desses\",\"desta\",\"destas\",\"deste\",\"deste\",\"destes\",\"deve\",\"devem\",\"devendo\",\"dever\",\"deverá\",\"deverão\",\"deveria\",\"deveriam\",\"devia\",\"deviam\",\"disse\",\"disso\",\"disto\",\"dito\",\n",
        "\"diz\",\"dizem\",\"do\",\"dos\",\"e\",\"é\",\"ela\",\"elas\",\"ele\",\"eles\",\"em\",\"enquanto\",\"entre\",\"era\",\"essa\",\"essas\",\"esse\",\"esses\",\"esta\",\"está\",\"estamos\",\"estão\",\"estas\",\"estava\",\"estavam\",\n",
        "\"estávamos\",\"este\",\"estes\",\"estou\",\"eu\",\"fazendo\",\"fazer\",\"feita\",\"feitas\",\"feito\",\"feitos\",\"foi\",\"for\",\"foram\",\"fosse\",\"fossem\",\"grande\",\"grandes\",\"há\",\"isso\",\"isto\",\"já\",\"la\",\"lá\",\n",
        "\"lhe\",\"lhes\",\"lo\",\"mas\",\"me\",\"mesma\",\"mesmas\",\"mesmo\",\"mesmos\",\"meu\",\"meus\",\"minha\",\"minhas\",\"muita\",\"muitas\",\"muito\",\"muitos\",\"na\",\"não\",\"nas\",\"nem\",\"nenhum\",\"nessa\",\"nessas\",\"nesta\",\n",
        "\"nestas\",\"ninguém\",\"no\",\"nos\",\"nós\",\"nossa\",\"nossas\",\"nosso\",\"nossos\",\"num\",\"numa\",\"nunca\",\"o\",\"os\",\"ou\",\"outra\",\"outras\",\"outro\",\"outros\",\"para\",\"pela\",\"pelas\",\"pelo\",\"pelos\",\"pequena\",\n",
        "\"pequenas\",\"pequeno\",\"pequenos\",\"per\",\"perante\",\"pode\",\"pude\",\"podendo\",\"poder\",\"poderia\",\"poderiam\",\"podia\",\"podiam\",\"pois\",\"por\",\"porém\",\"porque\",\"posso\",\"pouca\",\"poucas\",\"pouco\",\n",
        "\"poucos\",\"primeiro\",\"primeiros\",\"própria\",\"próprias\",\"próprio\",\"próprios\",\"quais\",\"qual\",\"quando\",\"quanto\",\"quantos\",\"que\",\"quem\",\"são\",\"se\",\"seja\",\"sejam\",\"sem\",\"sempre\",\"sendo\",\n",
        "\"será\",\"serão\",\"seu\",\"seus\",\"si\",\"sido\",\"só\",\"sob\",\"sobre\",\"sua\",\"suas\",\"talvez\",\"também\",\"tampouco\",\"te\",\"tem\",\"tendo\",\"tenha\",\"ter\",\"teu\",\"teus\",\"ti\",\"tido\",\"tinha\",\"tinham\",\"toda\",\n",
        "\"todas\",\"todavia\",\"todo\",\"todos\",\"tu\",\"tua\",\"tuas\",\"tudo\",\"última\",\"últimas\",\"último\",\"últimos\",\"um\",\"uma\",\"umas\",\"uns\",\"vendo\",\"ver\",\"vez\",\"vindo\",\"vir\",\"vos\",\"vós\",\"q\",\"pro\",\"pra\",\"vem\",\n",
        "\"pq\",\"você\",\"deu\",\"hj\",\"ta\",\"cep\",\"1kg\",\"necessidade\",\"não\",\"nao\",\"nesse\",\"aqui\",\"time\",\"mais\",\"tá\",\"aí\",\"vocês\",\"voces\",\"série\",\"serie\",\"faz\",\"ser\",\"cara\",\"fez\",\"dá\",\n",
        "\"dão\", \"dando\", \"dar\", \"demaiisssss\", \"demais\", \"demaisss\", \"demaissss\", \"demaissssss\", \"desda\", \"desde\", \"dia\", \"dias\", \"dois\", \"doisq\", \"duas\", \"eh\", \"ehh\", \"ehhhh\", \"então\", \"fala\", \"falado\",\n",
        "\"falador\", \"faladoum\", \"falam\", \"falamos\", \"falando\", \"falar\", \"falaram\", \"falarem\", \"falasse\", \"falastrão\", \"falava\", \"falavam\", \"falcão\", \"fale\", \"falei\", \"falou\", \"foto\", \"fotos\", \"hoje\",\n",
        "\"hojese\", \"hojetime\", \"hojr\", \"i\", \"ia\", \"iae\", \"ir\", \"ja\", \"joga\", \"jogávamos\", \"jogão\", \"jogaa\", \"jogaaar\", \"jogada\", \"jogadaça\", \"jogadas\", \"jogadasfaz\", \"jogadassaa\", \"jogadassas\", \"jogado\", \n",
        "\"jogador\", \"jogadorão\", \"jogadoras\", \"jogadores\", \"jogadoresmais\", \"jogadorzinho\", \"jogaf\", \"jogai\", \"jogam\", \"jogamos\", \"jogando\", \"jogar\", \"jogará\", \"jogaram\", \"jogarem\", \"jogarmonos\", \"jogarrrrr\", \n",
        "\"jogasso\", \"jogava\", \"jogndo\", \"jogo\", \"jogono\", \"jogopara\", \"jogopelo\", \"jogos\", \"jogou\", \"jogue\", \"joguei\", \"joguem\", \"joguin\", \"joguinho\", \"joguinhos\", \"junta\", \"juntar\", \"junto\", \n",
        "\"juntos\", \"juntoseshaylonnow\", \"juntosnobrasileirão\", \"mim\", \"msg\", \"msm\", \"mt\", \"mta\", \"mto\", \"mts\", \"mttt\", \"nada\", \"nadaÃ³timo\", \"nadaaaaa\", \"número\", \"números\", \"naquela\", \"naquele\", \"nasa\",\n",
        "\"nations\", \"nba\", \"nd\", \"ndd\", \"ne\", \"nela\", \"nelas\", \"nele\", \"neles\", \"nenhuma\", \"nenhumafica\", \"nenhuna\", \"nesses\", \"neste\", \"new\", \"ngm\", \"ninguem\", \"ninhum\", \"nisso\", \"nois\", \"noite\",\n",
        "\"noitee\", \"nome\", \"nomes\", \"numero\", \"numeros\", \"oh\", \"ohhh\", \"oi\", \"ok\", \"okay\", \"okkkkk\", \"ola\", \"olá\", \"olaf\", \"onde\", \"ops\", \"optou\", \"oq\", \"oqkkkkk\", \"oquanto\", \"oque\", \"p\", \"pix\", \n",
        "\"qd\", \"qdo\", \"qlq\", \"qm\", \"qnd\", \"qnts\", \"qq\", \"qqr\", \"qtos\", \"quão\", \"qué\", \"qualquer\", \"quanta\", \"quantas\", \"quer\", \"queríamos\", \"querem\", \"querer\", \"queres\", \"queria\", \"quero\", \"quis\", \n",
        "\"r\", \"ready\", \"rt\", \"semana\", \"semanal\", \"semanas\", \"silva\", \"sim\", \"simm\", \"simmm\", \"simmmm\", \"simmmmm\", \"sla\", \"slc\", \"slk\", \"small\", \"socorro\", \"sofri\", \"sofria\", \"sofrido\", \"sofridos\", \n",
        "\"sofrimento\", \"star\",  \"station\", \"stats\", \"still\", \"stop\", \"story\", \"tá\", \"tão\", \"tal\", \"tanta\", \"tantão\", \"tantas\", \"tanto\", \"tantooo\", \"tantos\", \"tao\", \"tava\", \"tavam\", \"tb\", \"tbem\", \"tbm\", \n",
        "\"tchê\", \"tchapordeus\", \"tchau\", \"tche\", \"td\", \"tdas\", \"tds\", \"than\", \"thanks\", \"that\", \"thats\", \"the\", \"their\", \"them\", \"then\", \"there\", \"they\", \"theyll\", \"third\", \"this\", \"thisgo\", \"thyere\", \n",
        "\"tik\", \"tiktok\", \"till\", \"tilt\", \"timing\", \"tl\", \"tlg\", \"tlgd\", \"tlvz\", \"tm\", \"tmb\", \"tmj\", \"tml\", \"to\", \"três\", \"trás\", \"tuddoooo\", \"tuddooooooooo\", \"tudooo\", \"tudoooo\", \"tudooooo\", \"tudoooooo\",\n",
        "\"tudooooooo\", \"tudoooooooo\", \"tudooooooooo\", \"tudooooooooooooo\", \"tudooooooooooooooo\", \"tudoooooooooooooooooooo\", \"tudoooooooooooooooooooooooooooooooooooo\", \"tuuudoooooo\", \"tuuuudo\", \n",
        "\"tuuuudoooooooo\", \"tuuuuuudoooooo\", \"tuuuuuudooooooo\", \"tuuuuuuuddddddooooooooo\", \"tuuuuuuuudddddddooooooooooo\", \"tuuuuuuuudoooooo\", \"tuuuuuuuuudo\", \"tuuuuuuuuuuuuuuuuuuudo\", \"tv\", \"tweet\", \"tweeta\",\n",
        "\"tweetar\",  \"tweets\", \"twit\", \"twitado\", \"twitar\", \"twitter\", \"vc\", \"vco\", \"vcs\", \"vdd\", \"very\", \"vi\", \"vlw\", \"voce\", \"wait\", \"wallpaper\", \"want\", \"wat\", \"way\", \"what\", \"whats\", \"whatsapp\", \"where\", \n",
        "\"win\", \"winners\", \"wins\", \"with\", \"woke\", \"wont\", \"worth\", \"year\", \"years\", \"yes\", \"yesssssss\", \"you\", \"your\", \"youth\", \"youtube\", \"youtuber\", \"youu\", \"zap\", \"dnv\", \"abel\", \"adriano\", \"alberto\", \n",
        "\"alex\", \"alexandrepato\", \"alexi\", \"alfredo\", \"alisson\", \"allyson\", \"allysoneventos\", \"alves\", \"alvessilvinho\", \"alyson\", \"alysson\", \"andré\", \"anual\", \"anuidade\", \"anula\", \"anulaaaaaa\", \"anulado\", \n",
        "\"anulando\", \"anular\", \"anule\", \"anulou\", \"arboleda\", \"arbolenda\", \"arejuela\", \"bandeira\", \"bandeirinha\", \"bruno\", \"brupedro\", \"bueno\", \"camacho\", \"cortez\", \"crespo\", \"cuca\", \"cucabol\", \"danilo\",\n",
        "\"davi\", \"david\", \"diego\", \"douglas\", \"douglascosta\", \"edenilso\", \"edenilsom\", \"edenilson\", \"ednilson\", \"felipão\", \"felipao\", \"felipe\", \"felipee\", \"felipemello\", \"felipemelonotimao\", \"fernandes\", \n",
        "\"fernandinho\", \"fernando\", \"fernandoasafil\", \"ferreira\", \"ferreirinha\", \"ferrer\", \"ferrerinha\", \"fred\", \"gabgol\", \"gabi\", \"gabigol\", \"gabinete\", \"gabriel\", \"galhardo\", \"galhardooooo\", \"galhargol\", \n",
        "\"galiotte\", \"ganso\", \"gerson\", \"gil\", \"gustavo\", \"gustavooo\", \"henriqu\", \"henrique\", \"hernández\", \"hernan\", \"hernandez\", \"hernane\", \"hernanes\", \"hernanesnÃ£o\", \"jean\", \"leo\", \"luan\", \"lucas\",\n",
        "\"luis\", \"luiz\", \"luizao\", \"mancini\", \"marcão\", \"marcelo\", \"marco\", \"marcos\", \"maria\", \"mariano\", \"marinho\", \"mario\", \"mariza\", \"marlon\", \"marques\", \"marquinhos\", \"marrony\", \"marti\", \"martinelli\",\n",
        "\"martorelli\", \"mateus\", \"mateusinho\", \"mateusinhoq\",  \"mateusvital\", \"matheu\", \"matheus\", \"matheusinho\", \"matheuzinho\", \"maucini\", \"maurício\", \"mauricio\", \"maxuell\", \"maxwell\", \"maycon\", \n",
        "\"mayke\", \"maykeo\", \"mazeti\", \"mazzeti\", \"mazzetti\", \"mello\", \"melo\", \"moisés\", \"neiyton\", \"nenê\", \"nene\", \"neneeee\", \"neneeeee\", \"neneeeeeeeeeeeeee\", \"nenene\", \"nunes\", \"orejeula\", \"orejuela\",\n",
        "\"orejuelaaaaaaaaaaaaaaaaaa\", \"orejuelaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"orejuella\", \"pablo\", \"pabloe\", \"pacheco\", \"pachecoooo\", \"pachecoooooo\", \"paolo\", \"pardabel\", \"pardal\", \"pato\", \"patric\", \n",
        "\"patrick\", \"patrik\", \"paulina\", \"paulinho\", \"paulino\", \"paulinos\", \"paulo\", \"pauloooo\", \"paulouilian\", \"paulovictornão\",  \"pedrinho\", \"pedro\", \"pedrocoisa\", \"pedroe\", \"pedroebeca\", \"pedrokkkk\", \n",
        "\"pepe\", \"picachu\", \"pikachu\", \"pikachuuuuu\", \"pikachuzado\", \"pyerre\", \"pyerreeee\", \"pyerreeeeeee\", \"rafael\", \"rafel\", \"rafinha\", \"ramí­rez\", \"ramires\", \"ramirez\", \"ramiro\", \"ramirojogador\", \"ramon\", \n",
        "\"ramos\", \"ramosgtedenilson\", \"raul\", \"ravanelli\", \"renato\", \"renatogauchonaselecao\", \"retardada\", \"retardado\", \"ribamar\", \"ribas\", \"ribeiro\", \"ricardin\", \"ricardinho\", \"ricardinhooooooooo\", \"ricardo\",\n",
        "\"ricargol\", \"richard\", \"richarlison\", \"robergol\", \"robershow\", \"roberson\", \"robert\", \"roberto\", \"roberts\", \"robertson\", \"roberty\", \"robertzzzzz\", \"robinho\", \"roblox\", \"robson\", \"rodi\", \n",
        "\"rodibala\", \"rodiii\", \"rodilindis\", \"rodilindo\", \"rodilindooo\", \"rodilindoooo\", \"rodilindooooo\", \"rodilindooooooooo\", \"rodilixo\", \"rodinei\", \"rodlindo\", \"rodnei\", \"rodolfo\", \"rodolfopara\", \"rodrigo\",\n",
        "\"rodriguéz\", \"rodrigues\", \"rodriguez\", \"rodriguinho\", \"rogério\", \"rogenio\", \"roger\", \"romário\", \"romarinho\", \"romario\", \"romero\", \"romildastico\", \"romildinho\", \"romildo\", \"ronaldao\", \n",
        "\"ronaldinho\", \"ronaldinhogaucho\", \"ronaldo\", \"roni\", \"ronielson\", \"ronny\", \"rony\", \"rooney\", \"rossi\", \"rossicley\", \"shaylon\", \"sylvinho\", \"sylvio\", \"taison\", \"talisson\", \"talles\", \"talleshernanes\", \n",
        "\"teixeira\", \"teixeiras\", \"thiago\", \"thiagol\", \"thiaguinho\", \"tinga\", \"valentim\", \"valentimm\", \"valentin\", \"vandeco\", \"vanderlei\", \"vanderson\", \"victor\", \"victoria\", \"victoro\", \"vital\", \"vitinho\",\n",
        "\"vito\", \"vitor\", \"vitu\", \"walter\", \"waltermas\", \"wanderson\", \"wel\", \"welington\", \"well\", \"wellington\", \"wellinton\", \"wendel\", \"wescley\", \"wesley\", \"weverton\", \"wiliam\", \"wililam\", \"will\", \"william\",\n",
        "\"willian\", \"willliam\", \"xavi\", \"xavier\", \"yago\", \"yashin\", \"ytalo\", \"yuri\", \"zé\", \"ze\", \"zico\", \"jhonata\", \"jhonatan\", \"jhonattan\", \"jhonnatan\", \"jhonny\", \"jhonota\", \"joão\", \"jóia\", \"joao\",\n",
        "\"lima\", \"muricy\", \"prass\", \"prasss\"]\n",
        "\n",
        "#\n",
        "# REMOVING STOPWORDS\n",
        "# \n",
        "\n",
        "#https://stackoverflow.com/questions/54366913/removing-stopwords-from-a-pandas-dataframe\n",
        "\n",
        "df_replies0['tweet_div'] = df_replies0.tweet_semmencoes.str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
        "df_replies0['tweet_semstop'] = df_replies0['tweet_div'].apply(lambda x: ' '.join([item for item in x.split() if item not in stopwords]))\n",
        "df_replies0.head(2000)\n",
        "\n",
        "# apagando as colunas desnecessárias\n",
        "\n",
        "del df_replies0['tweet_div']\n",
        "del df_replies0['tweet_semlinks']\n",
        "del df_replies0['tweet_semmencoes']\n",
        "del df_replies0['tweet_sememojis']\n",
        "\n",
        "df_replies0.info()\n",
        "\n",
        "#df_replies0.dropna(subset=['tweet_semstop'], inplace=True)\n",
        "\n",
        "df_replies0.head(2)\n",
        "\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "# ==================================== WORDCLOUDS\n",
        "# ==============================================================================================================================\n",
        "# ==============================================================================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt   # for wordclouds & charts\n",
        "import seaborn as sns   # for charts\n",
        "sns.set_style(\"whitegrid\");   # chart background style\n",
        "plt.rcParams['figure.dpi'] = 360   # for high res chart output\n",
        "from wordcloud import WordCloud   # for the wordcloud :)\n",
        "\n",
        "df_replies1 = df_replies0.nome_time.unique()\n",
        "df_replies1_5 = pd.DataFrame(df_replies1, columns=['nome_time'])\n",
        "\n",
        "# ==================================== CARREGANDO O ARQUIVO COM OS COLORMAPS ESPECÍFICOS DE CADA TME\n",
        "\n",
        "df_times_cores = pd.read_csv('/content/drive/My Drive/futebol_emocoes/times_do_brasileirao-cores.csv', error_bad_lines=False, sep=';')\n",
        "df_times_cores\n",
        "\n",
        "df_replies2 = pd.merge(df_replies1_5, df_times_cores, left_on=['nome_time'], right_on=['nome'], how='left')\n",
        "df_replies2\n",
        "\n",
        "appended_data = []\n",
        "\n",
        "for teams in df_replies2.itertuples():\n",
        "    nome                = getattr(teams, 'nome_time')\n",
        "    colormap            = getattr(teams, 'colormap')\n",
        "    li = list(nome.split(\",\"))\n",
        "    #nome = ['Cuiaba']\n",
        "    df_replies3= df_replies0[ (df_replies0.nome_time.isin(li)) ]\n",
        "    df_replies3\n",
        "  \n",
        "    df_freq = df_replies3.tweet_semstop.str.split(expand=True).stack().value_counts()\n",
        "\n",
        "    url = \"/content/drive/My Drive/futebol_emocoes/wordclouds/\"\n",
        "    ext   = '.png'\n",
        "    ext2   = '.csv'\n",
        "    hifen = \"_\"\n",
        "    freq = \"freq\"\n",
        "    rodada1 = \"rodada1\"\n",
        "    final_url = f'{url}{nome}{hifen}{rodada1}{ext}'\n",
        "\n",
        "    tweet_wordcloud = WordCloud(background_color=\"white\", width=380, height=380, colormap=colormap, max_words=200 ).generate_from_frequencies(df_freq)\n",
        "\n",
        "    tweet_wordcloud.to_file(final_url)\n",
        "\n",
        "    type(df_freq)\n",
        "    df_freq2 = pd.DataFrame(df_freq)\n",
        "    df_freq2['nome'] = getattr(teams, 'nome_time')\n",
        "    final_freq_url = f'{url}{nome}{hifen}{freq}{hifen}{rodada1}{ext2}'\n",
        "    \n",
        "    df_freq3 = df_freq2.reset_index()\n",
        "    df_freq3\n",
        "    df_freq3.columns.values[0] = \"palavra\"\n",
        "    df_freq3.columns.values[1] = \"freq\"\n",
        "    df_freq3.columns.values[2] = \"time\"\n",
        "\n",
        "    df_freq3.to_csv(final_freq_url)\n",
        "    \n",
        "    #print(d)\n",
        "    ooo = pd.DataFrame(df_freq3)\n",
        "    #print(ooo)\n",
        "    appended_data.append(ooo)\n",
        "\n",
        "df_words = pd.concat(appended_data)\n",
        "df_words\n",
        "\n",
        "df_words.to_csv('/content/drive/My Drive/futebol_emocoes/freq_palavras.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}